{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YenaChoi00/ML-Fianl-Project/blob/main/CIFAR10_Classification_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Final Project***\n",
        "---\n",
        "\n",
        "In this project, you will develop a convolutional neural network (CNN) to perform classification, and test it out on the CIFAR-10 dataset.\n",
        "\n",
        "\n",
        "The given code is a simple implement for the CIFAR-10 classification. \n",
        "\n",
        "The goal of the final project is to check whether you understand important concepts for training CNN such as:\n",
        "\n",
        "*   Network architecture\n",
        "*   Optimization\n",
        "*   Loss function\n",
        "*   Data preprocessing\n",
        "*   Regularization\n",
        "*   ...\n",
        "\n",
        "Therefore, you can **modify the given code to improve the performance** of the network **except for the dataset (i.e. trainset and testset)**.\n",
        "\n",
        "You should **upload your code and your report** including an explaination of your code and what you have changed or added."
      ],
      "metadata": {
        "id": "8H2mplXJU1d6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fr9zgCgCmdNq"
      },
      "outputs": [],
      "source": [
        "# AS usual, a bit of setup\n",
        "# If you need other libraries, you should import the libraries.\n",
        "\n",
        "import os, sys\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9pg5nB1pXEO",
        "outputId": "90b4b628-6446-4087-91d8-8bef53d7e677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Set the device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Loader**\n",
        "\n",
        "You can load the CIFAR-10 dataset using the library `torchvision.datasets`\n",
        "\n",
        "The details of CIFAR-10 dataset can be found in https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "`transforms_cifar10` is used to assemble several transforms for data preprossing."
      ],
      "metadata": {
        "id": "q-G1AJVsbqwc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOEFPBpcmmHU",
        "outputId": "fbeb3976-2429-4caa-b7e9-359ade8e5842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:06<00:00, 28162607.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transforms_cifar10 = transforms.Compose([transforms.Resize((32, 32)),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "                                 ])\n",
        "\n",
        "# Train dataset\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms_cifar10)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
        "\n",
        "# Test dataset\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms_cifar10)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "# Classes of CIFAR-10 dataset\n",
        "classes = (\"plane\", \"car\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "d7JdtMAzoCac",
        "outputId": "5bb82d4a-9c92-4cc6-eaf5-3d70f19a096a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " deer  frog  ship  bird\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Examples of dataset\n",
        "\n",
        "def imshow(img):\n",
        "  img = img /2 + 0.5\n",
        "  npimg = img.numpy()\n",
        "  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "imgs, labels = next(dataiter)\n",
        "\n",
        "imshow(torchvision.utils.make_grid(imgs))\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Your own CNN**\n",
        "\n",
        "You can implement your own network using libraries such as `torch.nn`and `torch.nn.functional`.\n",
        "\n",
        "`SimpleNet` and `VGG11` are examples to help your understand the implementation of the network.\n",
        "\n",
        "So, you can modify the given codes or create another awesome neural network for CIFAR-10 classification."
      ],
      "metadata": {
        "id": "Y77itjthc5Xh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "068wqyqdo_fc"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        # Conv2d -> ReLU -> Max pooling -> Conv2d -> ReLU -> Max pooling -> Fully Connected -> ReLU -> Fully Connected -> ReLU -> Fully Connected\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5) #(input channel, output channel, kernel_size, stride, padding)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  #(input features, output features)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = SimpleNet().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11oMT_o1mnL2"
      },
      "outputs": [],
      "source": [
        "# VGG Model\n",
        "\n",
        "cfg = {'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M']}\n",
        "\n",
        "class VGG11(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG11, self).__init__()\n",
        "        self.features = self.make_layers(cfg['VGG11'])\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU()]\n",
        "                in_channels = x\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "net = VGG11().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "\n",
        "# ResNet18을 위해 최대한 간단히 수정한 BasicBlock 클래스 정의\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        # 3x3 필터를 사용 (너비와 높이를 줄일 때는 stride 값 조절)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes) # 배치 정규화(batch normalization)\n",
        "\n",
        "        # 3x3 필터를 사용 (패딩을 1만큼 주기 때문에 너비와 높이가 동일)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes) # 배치 정규화(batch normalization)\n",
        "\n",
        "        self.shortcut = nn.Sequential() # identity인 경우\n",
        "        if stride != 1: # stride가 1이 아니라면, Identity mapping이 아닌 경우\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x) # (핵심) skip connection\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ResNet 클래스 정의\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        # 64개의 3x3 필터(filter)를 사용\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes # 다음 레이어를 위해 채널 수 변경\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "# ResNet18 함수 정의\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "net = ResNet18().to(device)"
      ],
      "metadata": {
        "id": "ks-WdDwj4Jv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loss function and optimizer**\n",
        "\n",
        "Set the **loss function and optimizer** for training CNN.\n",
        "You can modify the loss function or optimizer for better performance."
      ],
      "metadata": {
        "id": "SXLnGlLgdyZ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oB0f6C2npeoj"
      },
      "outputs": [],
      "source": [
        "# Loss function and optimizer\n",
        "#loss_fun = nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "net = torch.nn.DataParallel(net)\n",
        "cudnn.benchmark = True\n",
        "\n",
        "learning_rate = 0.1\n",
        "file_name = 'resnet18_cifar10.pt'\n",
        "\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.0002)\n",
        "\n",
        "def train(epoch):\n",
        "    print('\\n[ Train epoch: %d ]' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        benign_outputs = net(inputs)\n",
        "        loss = loss_fun(benign_outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = benign_outputs.max(1)\n",
        "\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print('\\nCurrent batch:', str(batch_idx))\n",
        "            print('Current benign train accuracy:', str(predicted.eq(targets).sum().item() / targets.size(0)))\n",
        "            print('Current benign train loss:', loss.item())\n",
        "\n",
        "    print('\\nTotal benign train accuarcy:', 100. * correct / total)\n",
        "    print('Total benign train loss:', train_loss)\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    print('\\n[ Test epoch: %d ]' % epoch)\n",
        "    net.eval()\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        total += targets.size(0)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss += loss_fun(outputs, targets).item()\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print('\\nTest accuarcy:', 100. * correct / total)\n",
        "    print('Test average loss:', loss / total)\n",
        "\n",
        "    state = {\n",
        "        'net': net.state_dict()\n",
        "    }\n",
        "    if not os.path.isdir('checkpoint'):\n",
        "        os.mkdir('checkpoint')\n",
        "    torch.save(state, './checkpoint/' + file_name)\n",
        "    print('Model Saved!')\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "    lr = learning_rate\n",
        "    if epoch >= 100:\n",
        "        lr /= 10\n",
        "    if epoch >= 150:\n",
        "        lr /= 10\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train the network**\n",
        "\n",
        "Train your own network using the above loss function and optimizer."
      ],
      "metadata": {
        "id": "DInMRCnReWno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(0, 5):\n",
        "    adjust_learning_rate(optimizer, epoch)\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih0E7c4bAkVp",
        "outputId": "77be957b-c388-440a-e6ce-8dc13c42c96b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ Train epoch: 0 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.0452094078063965\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.1476081609725952\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0116941928863525\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.22835348546504974\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.7911728620529175\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9696114659309387\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.102144718170166\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.158735752105713\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.870388984680176\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8952933549880981\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2358410358428955\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.323595404624939\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.48821452260017395\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5460553169250488\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5448134541511536\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.31696632504463196\n",
            "\n",
            "Current batch: 1600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.022839069366455\n",
            "\n",
            "Current batch: 1700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.8984354734420776\n",
            "\n",
            "Current batch: 1800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.5435826778411865\n",
            "\n",
            "Current batch: 1900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0096468925476074\n",
            "\n",
            "Current batch: 2000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9797183275222778\n",
            "\n",
            "Current batch: 2100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0555229187011719\n",
            "\n",
            "Current batch: 2200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0593758821487427\n",
            "\n",
            "Current batch: 2300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9769737720489502\n",
            "\n",
            "Current batch: 2400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.660959005355835\n",
            "\n",
            "Current batch: 2500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9889872074127197\n",
            "\n",
            "Current batch: 2600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.7841589450836182\n",
            "\n",
            "Current batch: 2700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.099419116973877\n",
            "\n",
            "Current batch: 2800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.557960569858551\n",
            "\n",
            "Current batch: 2900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8999793529510498\n",
            "\n",
            "Current batch: 3000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2625877857208252\n",
            "\n",
            "Current batch: 3100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.38261735439300537\n",
            "\n",
            "Current batch: 3200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1742873191833496\n",
            "\n",
            "Current batch: 3300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.42452454566955566\n",
            "\n",
            "Current batch: 3400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.366286039352417\n",
            "\n",
            "Current batch: 3500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6305928826332092\n",
            "\n",
            "Current batch: 3600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5786854028701782\n",
            "\n",
            "Current batch: 3700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.5431458950042725\n",
            "\n",
            "Current batch: 3800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9476275444030762\n",
            "\n",
            "Current batch: 3900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.4570051431655884\n",
            "\n",
            "Current batch: 4000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1122210025787354\n",
            "\n",
            "Current batch: 4100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0208170413970947\n",
            "\n",
            "Current batch: 4200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0348519086837769\n",
            "\n",
            "Current batch: 4300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.7158534526824951\n",
            "\n",
            "Current batch: 4400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.71742844581604\n",
            "\n",
            "Current batch: 4500\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.244493007659912\n",
            "\n",
            "Current batch: 4600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.043272018432617\n",
            "\n",
            "Current batch: 4700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.363820195198059\n",
            "\n",
            "Current batch: 4800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.840067982673645\n",
            "\n",
            "Current batch: 4900\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.30562907457351685\n",
            "\n",
            "Current batch: 5000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.3036260604858398\n",
            "\n",
            "Current batch: 5100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.1108081340789795\n",
            "\n",
            "Current batch: 5200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5480483770370483\n",
            "\n",
            "Current batch: 5300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7227948904037476\n",
            "\n",
            "Current batch: 5400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.304499626159668\n",
            "\n",
            "Current batch: 5500\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 2.2138452529907227\n",
            "\n",
            "Current batch: 5600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.5692925453186035\n",
            "\n",
            "Current batch: 5700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0650315284729004\n",
            "\n",
            "Current batch: 5800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9289513826370239\n",
            "\n",
            "Current batch: 5900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.1051020622253418\n",
            "\n",
            "Current batch: 6000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0679912567138672\n",
            "\n",
            "Current batch: 6100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.532841682434082\n",
            "\n",
            "Current batch: 6200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.3171108365058899\n",
            "\n",
            "Current batch: 6300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2277714014053345\n",
            "\n",
            "Current batch: 6400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.499767541885376\n",
            "\n",
            "Current batch: 6500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8315950632095337\n",
            "\n",
            "Current batch: 6600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.4615576267242432\n",
            "\n",
            "Current batch: 6700\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.2283716201782227\n",
            "\n",
            "Current batch: 6800\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.5478855967521667\n",
            "\n",
            "Current batch: 6900\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 3.2569665908813477\n",
            "\n",
            "Current batch: 7000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2856258153915405\n",
            "\n",
            "Current batch: 7100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.487204670906067\n",
            "\n",
            "Current batch: 7200\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 2.8659489154815674\n",
            "\n",
            "Current batch: 7300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.451812982559204\n",
            "\n",
            "Current batch: 7400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.34366375207901\n",
            "\n",
            "Current batch: 7500\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.4602370262145996\n",
            "\n",
            "Current batch: 7600\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.7255592346191406\n",
            "\n",
            "Current batch: 7700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5160431861877441\n",
            "\n",
            "Current batch: 7800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0057241916656494\n",
            "\n",
            "Current batch: 7900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6834993362426758\n",
            "\n",
            "Current batch: 8000\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.5768590569496155\n",
            "\n",
            "Current batch: 8100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8671642541885376\n",
            "\n",
            "Current batch: 8200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.4454028606414795\n",
            "\n",
            "Current batch: 8300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.8296546936035156\n",
            "\n",
            "Current batch: 8400\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.8069909811019897\n",
            "\n",
            "Current batch: 8500\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 2.862319231033325\n",
            "\n",
            "Current batch: 8600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7500345706939697\n",
            "\n",
            "Current batch: 8700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.5161885023117065\n",
            "\n",
            "Current batch: 8800\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.4483897686004639\n",
            "\n",
            "Current batch: 8900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.7997868061065674\n",
            "\n",
            "Current batch: 9000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9015634655952454\n",
            "\n",
            "Current batch: 9100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9539794325828552\n",
            "\n",
            "Current batch: 9200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7396538257598877\n",
            "\n",
            "Current batch: 9300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.5530569553375244\n",
            "\n",
            "Current batch: 9400\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.34304678440094\n",
            "\n",
            "Current batch: 9500\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.40895143151283264\n",
            "\n",
            "Current batch: 9600\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.2399326115846634\n",
            "\n",
            "Current batch: 9700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5584769248962402\n",
            "\n",
            "Current batch: 9800\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.39013931155204773\n",
            "\n",
            "Current batch: 9900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.7048051357269287\n",
            "\n",
            "Current batch: 10000\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 1.940479040145874\n",
            "\n",
            "Current batch: 10100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.6857658624649048\n",
            "\n",
            "Current batch: 10200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.5743026733398438\n",
            "\n",
            "Current batch: 10300\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.6202722787857056\n",
            "\n",
            "Current batch: 10400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.639924168586731\n",
            "\n",
            "Current batch: 10500\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.8357272148132324\n",
            "\n",
            "Current batch: 10600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.541101098060608\n",
            "\n",
            "Current batch: 10700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9056999683380127\n",
            "\n",
            "Current batch: 10800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0974242687225342\n",
            "\n",
            "Current batch: 10900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.051222801208496\n",
            "\n",
            "Current batch: 11000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0461589097976685\n",
            "\n",
            "Current batch: 11100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.87366783618927\n",
            "\n",
            "Current batch: 11200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9184234738349915\n",
            "\n",
            "Current batch: 11300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9536690711975098\n",
            "\n",
            "Current batch: 11400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.6229963302612305\n",
            "\n",
            "Current batch: 11500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9443701505661011\n",
            "\n",
            "Current batch: 11600\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.33940064907073975\n",
            "\n",
            "Current batch: 11700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8805975914001465\n",
            "\n",
            "Current batch: 11800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3023937940597534\n",
            "\n",
            "Current batch: 11900\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.8029090166091919\n",
            "\n",
            "Current batch: 12000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9481164813041687\n",
            "\n",
            "Current batch: 12100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.4476251006126404\n",
            "\n",
            "Current batch: 12200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.113422155380249\n",
            "\n",
            "Current batch: 12300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.3336362838745117\n",
            "\n",
            "Current batch: 12400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.5842086672782898\n",
            "\n",
            "Total benign train accuarcy: 56.09\n",
            "Total benign train loss: 15881.064124401659\n",
            "\n",
            "[ Train epoch: 1 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.3155287504196167\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.5218849182128906\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.23859751224517822\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6031997203826904\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.47028592228889465\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.750133216381073\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.7193927764892578\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.7704015374183655\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.19926530122756958\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0632201433181763\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7047577500343323\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6903876066207886\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3069707155227661\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.1901869773864746\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6496704816818237\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.121662974357605\n",
            "\n",
            "Current batch: 1600\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.41452229022979736\n",
            "\n",
            "Current batch: 1700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.2285432815551758\n",
            "\n",
            "Current batch: 1800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.4793033599853516\n",
            "\n",
            "Current batch: 1900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.6220903396606445\n",
            "\n",
            "Current batch: 2000\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 3.136195182800293\n",
            "\n",
            "Current batch: 2100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.029578685760498\n",
            "\n",
            "Current batch: 2200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.260424017906189\n",
            "\n",
            "Current batch: 2300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.8942645788192749\n",
            "\n",
            "Current batch: 2400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9636973142623901\n",
            "\n",
            "Current batch: 2500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.8660149574279785\n",
            "\n",
            "Current batch: 2600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.3563158512115479\n",
            "\n",
            "Current batch: 2700\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.2801283299922943\n",
            "\n",
            "Current batch: 2800\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.42175430059432983\n",
            "\n",
            "Current batch: 2900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.7395380139350891\n",
            "\n",
            "Current batch: 3000\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.0325417518615723\n",
            "\n",
            "Current batch: 3100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9377040863037109\n",
            "\n",
            "Current batch: 3200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6707297563552856\n",
            "\n",
            "Current batch: 3300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.23093761503696442\n",
            "\n",
            "Current batch: 3400\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.5302512645721436\n",
            "\n",
            "Current batch: 3500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9956895112991333\n",
            "\n",
            "Current batch: 3600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.162358283996582\n",
            "\n",
            "Current batch: 3700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.701404333114624\n",
            "\n",
            "Current batch: 3800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9176971912384033\n",
            "\n",
            "Current batch: 3900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.2344821691513062\n",
            "\n",
            "Current batch: 4000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.3390554189682007\n",
            "\n",
            "Current batch: 4100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7565075159072876\n",
            "\n",
            "Current batch: 4200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.693082869052887\n",
            "\n",
            "Current batch: 4300\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.6537654399871826\n",
            "\n",
            "Current batch: 4400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.063662052154541\n",
            "\n",
            "Current batch: 4500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2163976430892944\n",
            "\n",
            "Current batch: 4600\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.4983920454978943\n",
            "\n",
            "Current batch: 4700\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.2286940962076187\n",
            "\n",
            "Current batch: 4800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0301748514175415\n",
            "\n",
            "Current batch: 4900\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.1050559282302856\n",
            "\n",
            "Current batch: 5000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5894389748573303\n",
            "\n",
            "Current batch: 5100\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.2834341526031494\n",
            "\n",
            "Current batch: 5200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.2239016443490982\n",
            "\n",
            "Current batch: 5300\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.957601547241211\n",
            "\n",
            "Current batch: 5400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.48372378945350647\n",
            "\n",
            "Current batch: 5500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.7010817527770996\n",
            "\n",
            "Current batch: 5600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.49290183186531067\n",
            "\n",
            "Current batch: 5700\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.1022233963012695\n",
            "\n",
            "Current batch: 5800\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.4824829697608948\n",
            "\n",
            "Current batch: 5900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.4496347904205322\n",
            "\n",
            "Current batch: 6000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.085216999053955\n",
            "\n",
            "Current batch: 6100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.7429194450378418\n",
            "\n",
            "Current batch: 6200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6898878812789917\n",
            "\n",
            "Current batch: 6300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.43647581338882446\n",
            "\n",
            "Current batch: 6400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8035619258880615\n",
            "\n",
            "Current batch: 6500\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.3954904079437256\n",
            "\n",
            "Current batch: 6600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.1485835313796997\n",
            "\n",
            "Current batch: 6700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9451005458831787\n",
            "\n",
            "Current batch: 6800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7389740943908691\n",
            "\n",
            "Current batch: 6900\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.17141056060791\n",
            "\n",
            "Current batch: 7000\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.3728143870830536\n",
            "\n",
            "Current batch: 7100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3542537689208984\n",
            "\n",
            "Current batch: 7200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6094071269035339\n",
            "\n",
            "Current batch: 7300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.8679822087287903\n",
            "\n",
            "Current batch: 7400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.219693660736084\n",
            "\n",
            "Current batch: 7500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1780661344528198\n",
            "\n",
            "Current batch: 7600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.3778183460235596\n",
            "\n",
            "Current batch: 7700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6335115432739258\n",
            "\n",
            "Current batch: 7800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9298824071884155\n",
            "\n",
            "Current batch: 7900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2702115774154663\n",
            "\n",
            "Current batch: 8000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.585762083530426\n",
            "\n",
            "Current batch: 8100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.7998526692390442\n",
            "\n",
            "Current batch: 8200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8558359742164612\n",
            "\n",
            "Current batch: 8300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3743191957473755\n",
            "\n",
            "Current batch: 8400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.035300374031067\n",
            "\n",
            "Current batch: 8500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9842453002929688\n",
            "\n",
            "Current batch: 8600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3367822170257568\n",
            "\n",
            "Current batch: 8700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.1160672903060913\n",
            "\n",
            "Current batch: 8800\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.6051449179649353\n",
            "\n",
            "Current batch: 8900\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 2.638568878173828\n",
            "\n",
            "Current batch: 9000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.702568531036377\n",
            "\n",
            "Current batch: 9100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.4440462589263916\n",
            "\n",
            "Current batch: 9200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.530378818511963\n",
            "\n",
            "Current batch: 9300\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.4093663692474365\n",
            "\n",
            "Current batch: 9400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.6458181142807007\n",
            "\n",
            "Current batch: 9500\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.19877491891384125\n",
            "\n",
            "Current batch: 9600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.6508522033691406\n",
            "\n",
            "Current batch: 9700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.6783322095870972\n",
            "\n",
            "Current batch: 9800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.1142582893371582\n",
            "\n",
            "Current batch: 9900\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.534881591796875\n",
            "\n",
            "Current batch: 10000\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.924127459526062\n",
            "\n",
            "Current batch: 10100\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.069611668586731\n",
            "\n",
            "Current batch: 10200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.2764141857624054\n",
            "\n",
            "Current batch: 10300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.3576425313949585\n",
            "\n",
            "Current batch: 10400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.970721960067749\n",
            "\n",
            "Current batch: 10500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3115004301071167\n",
            "\n",
            "Current batch: 10600\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.6125010251998901\n",
            "\n",
            "Current batch: 10700\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.2136507034301758\n",
            "\n",
            "Current batch: 10800\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.142172336578369\n",
            "\n",
            "Current batch: 10900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.228352665901184\n",
            "\n",
            "Current batch: 11000\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.6321690082550049\n",
            "\n",
            "Current batch: 11100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5564101338386536\n",
            "\n",
            "Current batch: 11200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7928430438041687\n",
            "\n",
            "Current batch: 11300\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.9083882570266724\n",
            "\n",
            "Current batch: 11400\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.3559731245040894\n",
            "\n",
            "Current batch: 11500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.8449828028678894\n",
            "\n",
            "Current batch: 11600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3976540565490723\n",
            "\n",
            "Current batch: 11700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.6417020559310913\n",
            "\n",
            "Current batch: 11800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8054013252258301\n",
            "\n",
            "Current batch: 11900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.8607769012451172\n",
            "\n",
            "Current batch: 12000\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.5489108562469482\n",
            "\n",
            "Current batch: 12100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0307371616363525\n",
            "\n",
            "Current batch: 12200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.8082993030548096\n",
            "\n",
            "Current batch: 12300\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.085338830947876\n",
            "\n",
            "Current batch: 12400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1425514221191406\n",
            "\n",
            "Total benign train accuarcy: 57.154\n",
            "Total benign train loss: 15480.24448605813\n",
            "\n",
            "[ Train epoch: 2 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0719834566116333\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.6429250240325928\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7827224135398865\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.4392544627189636\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5335876941680908\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.518026351928711\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.110105276107788\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1662473678588867\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3761787414550781\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.7047929763793945\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.2113587856292725\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.9169601202011108\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8527765870094299\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3870244026184082\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.39396825432777405\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8023548126220703\n",
            "\n",
            "Current batch: 1600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.5385725498199463\n",
            "\n",
            "Current batch: 1700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.342050790786743\n",
            "\n",
            "Current batch: 1800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6808309555053711\n",
            "\n",
            "Current batch: 1900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6359800100326538\n",
            "\n",
            "Current batch: 2000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.7958358526229858\n",
            "\n",
            "Current batch: 2100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1246110200881958\n",
            "\n",
            "Current batch: 2200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8893696069717407\n",
            "\n",
            "Current batch: 2300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.6512339115142822\n",
            "\n",
            "Current batch: 2400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8525106310844421\n",
            "\n",
            "Current batch: 2500\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 2.0430984497070312\n",
            "\n",
            "Current batch: 2600\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.4758968651294708\n",
            "\n",
            "Current batch: 2700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0448671579360962\n",
            "\n",
            "Current batch: 2800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5969480276107788\n",
            "\n",
            "Current batch: 2900\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.802644968032837\n",
            "\n",
            "Current batch: 3000\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.2727633714675903\n",
            "\n",
            "Current batch: 3100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0680934190750122\n",
            "\n",
            "Current batch: 3200\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.2412760257720947\n",
            "\n",
            "Current batch: 3300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.6204023361206055\n",
            "\n",
            "Current batch: 3400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.9107542037963867\n",
            "\n",
            "Current batch: 3500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3809051513671875\n",
            "\n",
            "Current batch: 3600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6454891562461853\n",
            "\n",
            "Current batch: 3700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6773547530174255\n",
            "\n",
            "Current batch: 3800\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.5265846848487854\n",
            "\n",
            "Current batch: 3900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.4002175331115723\n",
            "\n",
            "Current batch: 4000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8494348526000977\n",
            "\n",
            "Current batch: 4100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9554786682128906\n",
            "\n",
            "Current batch: 4200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.38802456855773926\n",
            "\n",
            "Current batch: 4300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2454359531402588\n",
            "\n",
            "Current batch: 4400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6118702292442322\n",
            "\n",
            "Current batch: 4500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2804781198501587\n",
            "\n",
            "Current batch: 4600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.7465910911560059\n",
            "\n",
            "Current batch: 4700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0012619495391846\n",
            "\n",
            "Current batch: 4800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.708417534828186\n",
            "\n",
            "Current batch: 4900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0762114524841309\n",
            "\n",
            "Current batch: 5000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.4534640312194824\n",
            "\n",
            "Current batch: 5100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.3987235426902771\n",
            "\n",
            "Current batch: 5200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1899282932281494\n",
            "\n",
            "Current batch: 5300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2361376285552979\n",
            "\n",
            "Current batch: 5400\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.0296645164489746\n",
            "\n",
            "Current batch: 5500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.0192201137542725\n",
            "\n",
            "Current batch: 5600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8856680393218994\n",
            "\n",
            "Current batch: 5700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.5417765378952026\n",
            "\n",
            "Current batch: 5800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1362450122833252\n",
            "\n",
            "Current batch: 5900\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.643129825592041\n",
            "\n",
            "Current batch: 6000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.4862562417984009\n",
            "\n",
            "Current batch: 6100\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.5264573097229004\n",
            "\n",
            "Current batch: 6200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5298477411270142\n",
            "\n",
            "Current batch: 6300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.363503098487854\n",
            "\n",
            "Current batch: 6400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.5720471143722534\n",
            "\n",
            "Current batch: 6500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5541071891784668\n",
            "\n",
            "Current batch: 6600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.8689899444580078\n",
            "\n",
            "Current batch: 6700\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.549875795841217\n",
            "\n",
            "Current batch: 6800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.1689070463180542\n",
            "\n",
            "Current batch: 6900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.435332715511322\n",
            "\n",
            "Current batch: 7000\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.083841323852539\n",
            "\n",
            "Current batch: 7100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.419313669204712\n",
            "\n",
            "Current batch: 7200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.4238675832748413\n",
            "\n",
            "Current batch: 7300\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.2982592582702637\n",
            "\n",
            "Current batch: 7400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.46855640411376953\n",
            "\n",
            "Current batch: 7500\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.1514754593372345\n",
            "\n",
            "Current batch: 7600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.4442968368530273\n",
            "\n",
            "Current batch: 7700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.8966217041015625\n",
            "\n",
            "Current batch: 7800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.6458983421325684\n",
            "\n",
            "Current batch: 7900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.7923898696899414\n",
            "\n",
            "Current batch: 8000\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.4242886304855347\n",
            "\n",
            "Current batch: 8100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9749283790588379\n",
            "\n",
            "Current batch: 8200\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.5747246742248535\n",
            "\n",
            "Current batch: 8300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7967373132705688\n",
            "\n",
            "Current batch: 8400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.5959563255310059\n",
            "\n",
            "Current batch: 8500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7304813265800476\n",
            "\n",
            "Current batch: 8600\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.27069759368896484\n",
            "\n",
            "Current batch: 8700\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.40531137585639954\n",
            "\n",
            "Current batch: 8800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.4997731447219849\n",
            "\n",
            "Current batch: 8900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.671750783920288\n",
            "\n",
            "Current batch: 9000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.875271201133728\n",
            "\n",
            "Current batch: 9100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3201489448547363\n",
            "\n",
            "Current batch: 9200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.7149858474731445\n",
            "\n",
            "Current batch: 9300\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.7710803747177124\n",
            "\n",
            "Current batch: 9400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.8153282403945923\n",
            "\n",
            "Current batch: 9500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.0788726806640625\n",
            "\n",
            "Current batch: 9600\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.1558018922805786\n",
            "\n",
            "Current batch: 9700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3446295261383057\n",
            "\n",
            "Current batch: 9800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.1580374240875244\n",
            "\n",
            "Current batch: 9900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.4990482032299042\n",
            "\n",
            "Current batch: 10000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.377000093460083\n",
            "\n",
            "Current batch: 10100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3519880771636963\n",
            "\n",
            "Current batch: 10200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7645435929298401\n",
            "\n",
            "Current batch: 10300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0441875457763672\n",
            "\n",
            "Current batch: 10400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5611131191253662\n",
            "\n",
            "Current batch: 10500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1040730476379395\n",
            "\n",
            "Current batch: 10600\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.6395381689071655\n",
            "\n",
            "Current batch: 10700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0131394863128662\n",
            "\n",
            "Current batch: 10800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.1284104585647583\n",
            "\n",
            "Current batch: 10900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.1860374212265015\n",
            "\n",
            "Current batch: 11000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9778135418891907\n",
            "\n",
            "Current batch: 11100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.4996557235717773\n",
            "\n",
            "Current batch: 11200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.2228228896856308\n",
            "\n",
            "Current batch: 11300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.7994195222854614\n",
            "\n",
            "Current batch: 11400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.4349057972431183\n",
            "\n",
            "Current batch: 11500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6999326944351196\n",
            "\n",
            "Current batch: 11600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7158607840538025\n",
            "\n",
            "Current batch: 11700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.226625680923462\n",
            "\n",
            "Current batch: 11800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9613717794418335\n",
            "\n",
            "Current batch: 11900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5442600846290588\n",
            "\n",
            "Current batch: 12000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5644046068191528\n",
            "\n",
            "Current batch: 12100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.4463355541229248\n",
            "\n",
            "Current batch: 12200\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.2464792728424072\n",
            "\n",
            "Current batch: 12300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.1773791313171387\n",
            "\n",
            "Current batch: 12400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.268544316291809\n",
            "\n",
            "Total benign train accuarcy: 58.114\n",
            "Total benign train loss: 15186.463235272095\n",
            "\n",
            "[ Train epoch: 3 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5358952879905701\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9648377299308777\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.5091438293457031\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9268000721931458\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.9069584608078003\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.3471918106079102\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.2379727363586426\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.146005630493164\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.4752376079559326\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7753100395202637\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.46333521604537964\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 2.1206347942352295\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9965411424636841\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8083676099777222\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.8541333675384521\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.6667997241020203\n",
            "\n",
            "Current batch: 1600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.7224535942077637\n",
            "\n",
            "Current batch: 1700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6679818630218506\n",
            "\n",
            "Current batch: 1800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2382007837295532\n",
            "\n",
            "Current batch: 1900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1645647287368774\n",
            "\n",
            "Current batch: 2000\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.2850036025047302\n",
            "\n",
            "Current batch: 2100\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 1.6975977420806885\n",
            "\n",
            "Current batch: 2200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.4190767705440521\n",
            "\n",
            "Current batch: 2300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6809754371643066\n",
            "\n",
            "Current batch: 2400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7211202383041382\n",
            "\n",
            "Current batch: 2500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8992822766304016\n",
            "\n",
            "Current batch: 2600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.209395408630371\n",
            "\n",
            "Current batch: 2700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9179980754852295\n",
            "\n",
            "Current batch: 2800\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.6087173223495483\n",
            "\n",
            "Current batch: 2900\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.31626802682876587\n",
            "\n",
            "Current batch: 3000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.902784526348114\n",
            "\n",
            "Current batch: 3100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.5100007057189941\n",
            "\n",
            "Current batch: 3200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1590882539749146\n",
            "\n",
            "Current batch: 3300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0156126022338867\n",
            "\n",
            "Current batch: 3400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 2.017274856567383\n",
            "\n",
            "Current batch: 3500\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.0753681659698486\n",
            "\n",
            "Current batch: 3600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.3897408246994019\n",
            "\n",
            "Current batch: 3700\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.190669298171997\n",
            "\n",
            "Current batch: 3800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.2371469736099243\n",
            "\n",
            "Current batch: 3900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.5511223077774048\n",
            "\n",
            "Current batch: 4000\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 2.373652935028076\n",
            "\n",
            "Current batch: 4100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8120564818382263\n",
            "\n",
            "Current batch: 4200\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 1.7440335750579834\n",
            "\n",
            "Current batch: 4300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6833053231239319\n",
            "\n",
            "Current batch: 4400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.4998222589492798\n",
            "\n",
            "Current batch: 4500\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 3.409759998321533\n",
            "\n",
            "Current batch: 4600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.5294034481048584\n",
            "\n",
            "Current batch: 4700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0541844367980957\n",
            "\n",
            "Current batch: 4800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.3364384174346924\n",
            "\n",
            "Current batch: 4900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1880500316619873\n",
            "\n",
            "Current batch: 5000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2891565561294556\n",
            "\n",
            "Current batch: 5100\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.258821964263916\n",
            "\n",
            "Current batch: 5200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.42305314540863037\n",
            "\n",
            "Current batch: 5300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6827737092971802\n",
            "\n",
            "Current batch: 5400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.2882254719734192\n",
            "\n",
            "Current batch: 5500\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.163745880126953\n",
            "\n",
            "Current batch: 5600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7869613766670227\n",
            "\n",
            "Current batch: 5700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.5534798502922058\n",
            "\n",
            "Current batch: 5800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7569882273674011\n",
            "\n",
            "Current batch: 5900\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.0259997844696045\n",
            "\n",
            "Current batch: 6000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.283258318901062\n",
            "\n",
            "Current batch: 6100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.6996396780014038\n",
            "\n",
            "Current batch: 6200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.46947264671325684\n",
            "\n",
            "Current batch: 6300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8483274579048157\n",
            "\n",
            "Current batch: 6400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.17378650605678558\n",
            "\n",
            "Current batch: 6500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.2353507280349731\n",
            "\n",
            "Current batch: 6600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3654448986053467\n",
            "\n",
            "Current batch: 6700\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.5729330778121948\n",
            "\n",
            "Current batch: 6800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.195414662361145\n",
            "\n",
            "Current batch: 6900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5021334290504456\n",
            "\n",
            "Current batch: 7000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7488058805465698\n",
            "\n",
            "Current batch: 7100\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.4538021087646484\n",
            "\n",
            "Current batch: 7200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0455052852630615\n",
            "\n",
            "Current batch: 7300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0589852333068848\n",
            "\n",
            "Current batch: 7400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.8958073854446411\n",
            "\n",
            "Current batch: 7500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.8279168605804443\n",
            "\n",
            "Current batch: 7600\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.2930741608142853\n",
            "\n",
            "Current batch: 7700\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 1.9843350648880005\n",
            "\n",
            "Current batch: 7800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2493263483047485\n",
            "\n",
            "Current batch: 7900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.8973264694213867\n",
            "\n",
            "Current batch: 8000\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.356133222579956\n",
            "\n",
            "Current batch: 8100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.2413325309753418\n",
            "\n",
            "Current batch: 8200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9922251105308533\n",
            "\n",
            "Current batch: 8300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6380239725112915\n",
            "\n",
            "Current batch: 8400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2883028984069824\n",
            "\n",
            "Current batch: 8500\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.29547184705734253\n",
            "\n",
            "Current batch: 8600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.1372175216674805\n",
            "\n",
            "Current batch: 8700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.1563655138015747\n",
            "\n",
            "Current batch: 8800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0877978801727295\n",
            "\n",
            "Current batch: 8900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.1188900470733643\n",
            "\n",
            "Current batch: 9000\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.2319705486297607\n",
            "\n",
            "Current batch: 9100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6406099200248718\n",
            "\n",
            "Current batch: 9200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.7904295921325684\n",
            "\n",
            "Current batch: 9300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9427605271339417\n",
            "\n",
            "Current batch: 9400\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.5889599323272705\n",
            "\n",
            "Current batch: 9500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.948469638824463\n",
            "\n",
            "Current batch: 9600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.7940431833267212\n",
            "\n",
            "Current batch: 9700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.601723849773407\n",
            "\n",
            "Current batch: 9800\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.3172144889831543\n",
            "\n",
            "Current batch: 9900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.8992533683776855\n",
            "\n",
            "Current batch: 10000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.8385990858078003\n",
            "\n",
            "Current batch: 10100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.32100072503089905\n",
            "\n",
            "Current batch: 10200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.24504318833351135\n",
            "\n",
            "Current batch: 10300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.7434604167938232\n",
            "\n",
            "Current batch: 10400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8909848928451538\n",
            "\n",
            "Current batch: 10500\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.051990509033203\n",
            "\n",
            "Current batch: 10600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.5791761875152588\n",
            "\n",
            "Current batch: 10700\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.2060022354125977\n",
            "\n",
            "Current batch: 10800\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.8591852188110352\n",
            "\n",
            "Current batch: 10900\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.5274509191513062\n",
            "\n",
            "Current batch: 11000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6641308665275574\n",
            "\n",
            "Current batch: 11100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9847798943519592\n",
            "\n",
            "Current batch: 11200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6675031185150146\n",
            "\n",
            "Current batch: 11300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0472209453582764\n",
            "\n",
            "Current batch: 11400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9054676294326782\n",
            "\n",
            "Current batch: 11500\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 1.8709158897399902\n",
            "\n",
            "Current batch: 11600\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 2.2244672775268555\n",
            "\n",
            "Current batch: 11700\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.5166066288948059\n",
            "\n",
            "Current batch: 11800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0789250135421753\n",
            "\n",
            "Current batch: 11900\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.6230692863464355\n",
            "\n",
            "Current batch: 12000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2347766160964966\n",
            "\n",
            "Current batch: 12100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.7058215141296387\n",
            "\n",
            "Current batch: 12200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.6294831037521362\n",
            "\n",
            "Current batch: 12300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.984844446182251\n",
            "\n",
            "Current batch: 12400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8416934609413147\n",
            "\n",
            "Total benign train accuarcy: 58.316\n",
            "Total benign train loss: 15083.478000150993\n",
            "\n",
            "[ Train epoch: 4 ]\n",
            "\n",
            "Current batch: 0\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1304430961608887\n",
            "\n",
            "Current batch: 100\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.7873055934906006\n",
            "\n",
            "Current batch: 200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5192927718162537\n",
            "\n",
            "Current batch: 300\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.7464418411254883\n",
            "\n",
            "Current batch: 400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.42411470413208\n",
            "\n",
            "Current batch: 500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.6379063129425049\n",
            "\n",
            "Current batch: 600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0295062065124512\n",
            "\n",
            "Current batch: 700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6361854076385498\n",
            "\n",
            "Current batch: 800\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.3413139283657074\n",
            "\n",
            "Current batch: 900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.8065462112426758\n",
            "\n",
            "Current batch: 1000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6745659708976746\n",
            "\n",
            "Current batch: 1100\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.3271336555480957\n",
            "\n",
            "Current batch: 1200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7744449377059937\n",
            "\n",
            "Current batch: 1300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.4155274629592896\n",
            "\n",
            "Current batch: 1400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9657747745513916\n",
            "\n",
            "Current batch: 1500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.150757908821106\n",
            "\n",
            "Current batch: 1600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5796493887901306\n",
            "\n",
            "Current batch: 1700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9980791807174683\n",
            "\n",
            "Current batch: 1800\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.4231046438217163\n",
            "\n",
            "Current batch: 1900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6361241340637207\n",
            "\n",
            "Current batch: 2000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.4819813966751099\n",
            "\n",
            "Current batch: 2100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.4314260482788086\n",
            "\n",
            "Current batch: 2200\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.7195720672607422\n",
            "\n",
            "Current batch: 2300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.18271826207637787\n",
            "\n",
            "Current batch: 2400\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.28817471861839294\n",
            "\n",
            "Current batch: 2500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8235269784927368\n",
            "\n",
            "Current batch: 2600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0159991979599\n",
            "\n",
            "Current batch: 2700\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.3161945939064026\n",
            "\n",
            "Current batch: 2800\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.2461047172546387\n",
            "\n",
            "Current batch: 2900\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.5732543468475342\n",
            "\n",
            "Current batch: 3000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9626956582069397\n",
            "\n",
            "Current batch: 3100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6795597076416016\n",
            "\n",
            "Current batch: 3200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9416964054107666\n",
            "\n",
            "Current batch: 3300\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.036566734313965\n",
            "\n",
            "Current batch: 3400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.1050548553466797\n",
            "\n",
            "Current batch: 3500\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.6374238133430481\n",
            "\n",
            "Current batch: 3600\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0463416576385498\n",
            "\n",
            "Current batch: 3700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2324447631835938\n",
            "\n",
            "Current batch: 3800\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.1248047351837158\n",
            "\n",
            "Current batch: 3900\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 3.6297903060913086\n",
            "\n",
            "Current batch: 4000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9071069955825806\n",
            "\n",
            "Current batch: 4100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.46135491132736206\n",
            "\n",
            "Current batch: 4200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6661255359649658\n",
            "\n",
            "Current batch: 4300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8724578022956848\n",
            "\n",
            "Current batch: 4400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.8321151733398438\n",
            "\n",
            "Current batch: 4500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.243341326713562\n",
            "\n",
            "Current batch: 4600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.602690577507019\n",
            "\n",
            "Current batch: 4700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.475475549697876\n",
            "\n",
            "Current batch: 4800\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 2.162452459335327\n",
            "\n",
            "Current batch: 4900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.289452075958252\n",
            "\n",
            "Current batch: 5000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7278048396110535\n",
            "\n",
            "Current batch: 5100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.7614409923553467\n",
            "\n",
            "Current batch: 5200\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.6832256317138672\n",
            "\n",
            "Current batch: 5300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7617443203926086\n",
            "\n",
            "Current batch: 5400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6892940402030945\n",
            "\n",
            "Current batch: 5500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8216049671173096\n",
            "\n",
            "Current batch: 5600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.3593988418579102\n",
            "\n",
            "Current batch: 5700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.6940457820892334\n",
            "\n",
            "Current batch: 5800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8694432973861694\n",
            "\n",
            "Current batch: 5900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.465741753578186\n",
            "\n",
            "Current batch: 6000\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.2581820487976074\n",
            "\n",
            "Current batch: 6100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.343156337738037\n",
            "\n",
            "Current batch: 6200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6654146909713745\n",
            "\n",
            "Current batch: 6300\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.4843997955322266\n",
            "\n",
            "Current batch: 6400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.013843297958374\n",
            "\n",
            "Current batch: 6500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0693621635437012\n",
            "\n",
            "Current batch: 6600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.8693554401397705\n",
            "\n",
            "Current batch: 6700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.4189002513885498\n",
            "\n",
            "Current batch: 6800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6205580830574036\n",
            "\n",
            "Current batch: 6900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9888026714324951\n",
            "\n",
            "Current batch: 7000\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.945425033569336\n",
            "\n",
            "Current batch: 7100\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.6691731810569763\n",
            "\n",
            "Current batch: 7200\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.6728545427322388\n",
            "\n",
            "Current batch: 7300\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.5722562074661255\n",
            "\n",
            "Current batch: 7400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.925160825252533\n",
            "\n",
            "Current batch: 7500\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.31088703870773315\n",
            "\n",
            "Current batch: 7600\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.17313286662101746\n",
            "\n",
            "Current batch: 7700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7909184098243713\n",
            "\n",
            "Current batch: 7800\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.4174352288246155\n",
            "\n",
            "Current batch: 7900\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.4002688229084015\n",
            "\n",
            "Current batch: 8000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.1397149562835693\n",
            "\n",
            "Current batch: 8100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0886248350143433\n",
            "\n",
            "Current batch: 8200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7500385046005249\n",
            "\n",
            "Current batch: 8300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.5763641595840454\n",
            "\n",
            "Current batch: 8400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8484100699424744\n",
            "\n",
            "Current batch: 8500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8094033002853394\n",
            "\n",
            "Current batch: 8600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.0710177421569824\n",
            "\n",
            "Current batch: 8700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.8388193845748901\n",
            "\n",
            "Current batch: 8800\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.5592045783996582\n",
            "\n",
            "Current batch: 8900\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.0060055255889893\n",
            "\n",
            "Current batch: 9000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.7240464687347412\n",
            "\n",
            "Current batch: 9100\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6516975164413452\n",
            "\n",
            "Current batch: 9200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.471158742904663\n",
            "\n",
            "Current batch: 9300\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.8530080318450928\n",
            "\n",
            "Current batch: 9400\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.14221453666687\n",
            "\n",
            "Current batch: 9500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.194185495376587\n",
            "\n",
            "Current batch: 9600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.2300615310668945\n",
            "\n",
            "Current batch: 9700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.6875165700912476\n",
            "\n",
            "Current batch: 9800\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.7980499267578125\n",
            "\n",
            "Current batch: 9900\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.19579005241394043\n",
            "\n",
            "Current batch: 10000\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5547749996185303\n",
            "\n",
            "Current batch: 10100\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 2.942812442779541\n",
            "\n",
            "Current batch: 10200\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.628619909286499\n",
            "\n",
            "Current batch: 10300\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.0979156494140625\n",
            "\n",
            "Current batch: 10400\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.5962530970573425\n",
            "\n",
            "Current batch: 10500\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 1.5082664489746094\n",
            "\n",
            "Current batch: 10600\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.127826452255249\n",
            "\n",
            "Current batch: 10700\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.8666359782218933\n",
            "\n",
            "Current batch: 10800\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.6395171880722046\n",
            "\n",
            "Current batch: 10900\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.21158048510551453\n",
            "\n",
            "Current batch: 11000\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 2.6132302284240723\n",
            "\n",
            "Current batch: 11100\n",
            "Current benign train accuracy: 0.0\n",
            "Current benign train loss: 2.3320019245147705\n",
            "\n",
            "Current batch: 11200\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.61659574508667\n",
            "\n",
            "Current batch: 11300\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.337036371231079\n",
            "\n",
            "Current batch: 11400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.3354250192642212\n",
            "\n",
            "Current batch: 11500\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 0.9779393076896667\n",
            "\n",
            "Current batch: 11600\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.410139799118042\n",
            "\n",
            "Current batch: 11700\n",
            "Current benign train accuracy: 0.75\n",
            "Current benign train loss: 0.9110129475593567\n",
            "\n",
            "Current batch: 11800\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.817107915878296\n",
            "\n",
            "Current batch: 11900\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.3135315477848053\n",
            "\n",
            "Current batch: 12000\n",
            "Current benign train accuracy: 0.25\n",
            "Current benign train loss: 1.4717786312103271\n",
            "\n",
            "Current batch: 12100\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.6805260181427002\n",
            "\n",
            "Current batch: 12200\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 2.2980470657348633\n",
            "\n",
            "Current batch: 12300\n",
            "Current benign train accuracy: 1.0\n",
            "Current benign train loss: 0.5194233655929565\n",
            "\n",
            "Current batch: 12400\n",
            "Current benign train accuracy: 0.5\n",
            "Current benign train loss: 1.5921746492385864\n",
            "\n",
            "Total benign train accuarcy: 58.848\n",
            "Total benign train loss: 14928.254662360996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test the network**\n",
        "\n",
        "Test the trained network using the testset.\n",
        "\n",
        "Accuracy of the network on the 10,000 test images is the final accuracy of your network. \n",
        "\n",
        "The closer the accuray is to 100%, the better the network classifies the input image."
      ],
      "metadata": {
        "id": "CgDA6_LlfNhE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "fNfptwqLqpdA",
        "outputId": "2b760e2c-923f-4f0a-dd39-bd9b226260af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GroundTruth:    cat  ship  ship plane\n",
            "Predicted:    cat  ship  ship  ship\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Test the trained model with sample\n",
        "\n",
        "dataiter_test = iter(testloader)\n",
        "img_test, labels_test = next(dataiter_test)\n",
        "\n",
        "imshow(torchvision.utils.make_grid(img_test))\n",
        "print('GroundTruth: ', ' '.join('%5s' % classes[labels_test[j]] for j in range(4)))\n",
        "\n",
        "img_test = img_test.to(device)\n",
        "labels_test = labels_test.to(device)\n",
        "\n",
        "# Prediction\n",
        "outputs_test = net(img_test)\n",
        "_, predicted = torch.max(outputs_test.data, 1)\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(4)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNrXRT1NrJft",
        "outputId": "300c9adb-8ad3-4527-b21b-b1c345ff2d73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10,000 test images: 53 %\n"
          ]
        }
      ],
      "source": [
        "# Test the trained model with overall test dataset\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "for data in testloader:\n",
        "    # Load the data\n",
        "    inputs_test, labels_test = data\n",
        "    inputs_test = inputs_test.to(device)\n",
        "    labels_test = labels_test.to(device)\n",
        "\n",
        "    # Estimate the output using the trained network\n",
        "    outputs_test = net(inputs_test)\n",
        "    _, predicted = torch.max(outputs_test.data, 1)\n",
        "    \n",
        "    # Calculate the accuracy\n",
        "    total += labels_test.size(0)\n",
        "    correct += (predicted == labels_test).sum()\n",
        "\n",
        "# Final accuracy\n",
        "print('Accuracy of the network on the 10,000 test images: %d %%' % (100 * correct / total))\n",
        "\n",
        "\n",
        "## [SimpleNet / Training 5 epochs] Accuracy of the network on the 10,000 test images: 9 %\n",
        "## [VGG11 / Training 5 epochs] Accuracy of the network on the 10,000 test images: 11 %"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}